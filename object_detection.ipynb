{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6e7306ac-7fda-4083-9e04-b8d570cd8ce7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-27 05:38:44.216615: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-02-27 05:38:44.231352: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1740634724.250029    7092 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1740634724.255485    7092 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-02-27 05:38:44.273772: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.18.0\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "import tempfile\n",
    "\n",
    "from IPython import display\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "import orbit\n",
    "import tensorflow_models as tfm\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "from official.core import exp_factory\n",
    "from official.core import config_definitions as cfg\n",
    "from official.vision.serving import export_saved_model_lib\n",
    "from official.vision.ops.preprocess_ops import normalize_image\n",
    "from official.vision.ops.preprocess_ops import resize_and_crop_image\n",
    "from official.vision.utils.object_detection import visualization_utils\n",
    "from official.vision.dataloaders.tf_example_decoder import TfExampleDecoder\n",
    "\n",
    "pp = pprint.PrettyPrinter(indent=4) # Set Pretty Print Indentation\n",
    "print(tf.__version__) # Check the version of tensorflow used\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c8b33030",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "SHUFFLE = 1024\n",
    "NUM_CLASSES = 10\n",
    "\n",
    "\n",
    "IMG_SIZE = [128,128,3]\n",
    "\n",
    "train_data_input_path = '/home/atallah/tensorflow_datasets/cifar10/3.0.2/cifar10-train.tfrecord-00000-of-00001'\n",
    "valid_data_input_path = '/home/atallah/tensorflow_datasets/cifar10/3.0.2/cifar10-test.tfrecord-00000-of-00001'\n",
    "model_dir = './trained_model/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "701c8f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_config = exp_factory.get_exp_config('retinanet_resnetfpn_coco')\n",
    "\n",
    "# Backbone config.\n",
    "exp_config.task.freeze_backbone = False\n",
    "exp_config.task.annotation_file = ''\n",
    "\n",
    "# Model config.\n",
    "exp_config.task.model.input_size = IMG_SIZE\n",
    "exp_config.task.model.num_classes = NUM_CLASSES + 1\n",
    "exp_config.task.model.detection_generator.tflite_post_processing.max_classes_per_detection = exp_config.task.model.num_classes\n",
    "\n",
    "# Training data config.\n",
    "exp_config.task.train_data.input_path = train_data_input_path\n",
    "exp_config.task.train_data.dtype = 'float32'\n",
    "exp_config.task.train_data.global_batch_size = BATCH_SIZE\n",
    "exp_config.task.train_data.parser.aug_scale_max = 1.0\n",
    "exp_config.task.train_data.parser.aug_scale_min = 1.0\n",
    "\n",
    "# Validation data config.\n",
    "exp_config.task.validation_data.input_path = valid_data_input_path\n",
    "exp_config.task.validation_data.dtype = 'float32'\n",
    "exp_config.task.validation_data.global_batch_size = BATCH_SIZE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4159dc89",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_steps = 1000\n",
    "exp_config.trainer.steps_per_loop = 100 # steps_per_loop = num_of_training_examples // train_batch_size\n",
    "\n",
    "exp_config.trainer.summary_interval = 100\n",
    "exp_config.trainer.checkpoint_interval = 100\n",
    "exp_config.trainer.validation_interval = 100\n",
    "exp_config.trainer.validation_steps =  100 # validation_steps = num_of_validation_examples // eval_batch_size\n",
    "exp_config.trainer.train_steps = train_steps\n",
    "exp_config.trainer.optimizer_config.warmup.linear.warmup_steps = 100\n",
    "exp_config.trainer.optimizer_config.learning_rate.type = 'cosine'\n",
    "exp_config.trainer.optimizer_config.learning_rate.cosine.decay_steps = train_steps\n",
    "exp_config.trainer.optimizer_config.learning_rate.cosine.initial_learning_rate = 0.1\n",
    "exp_config.trainer.optimizer_config.warmup.linear.warmup_learning_rate = 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2e85c722",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{   'runtime': {   'all_reduce_alg': None,\n",
      "                   'batchnorm_spatial_persistent': False,\n",
      "                   'dataset_num_private_threads': None,\n",
      "                   'default_shard_dim': -1,\n",
      "                   'distribution_strategy': 'mirrored',\n",
      "                   'enable_xla': False,\n",
      "                   'gpu_thread_mode': None,\n",
      "                   'loss_scale': None,\n",
      "                   'mixed_precision_dtype': 'bfloat16',\n",
      "                   'num_cores_per_replica': 1,\n",
      "                   'num_gpus': 0,\n",
      "                   'num_packs': 1,\n",
      "                   'per_gpu_thread_count': 0,\n",
      "                   'run_eagerly': False,\n",
      "                   'task_index': -1,\n",
      "                   'tpu': None,\n",
      "                   'tpu_enable_xla_dynamic_padder': None,\n",
      "                   'use_tpu_mp_strategy': False,\n",
      "                   'worker_hosts': None},\n",
      "    'task': {   'allow_image_summary': False,\n",
      "                'annotation_file': '',\n",
      "                'differential_privacy_config': None,\n",
      "                'export_config': {   'cast_detection_classes_to_float': False,\n",
      "                                     'cast_num_detections_to_float': False,\n",
      "                                     'output_intermediate_features': False,\n",
      "                                     'output_normalized_coordinates': False},\n",
      "                'freeze_backbone': False,\n",
      "                'init_checkpoint': 'gs://cloud-tpu-checkpoints/vision-2.0/resnet50_imagenet/ckpt-28080',\n",
      "                'init_checkpoint_modules': 'backbone',\n",
      "                'losses': {   'box_loss_weight': 50,\n",
      "                              'focal_loss_alpha': 0.25,\n",
      "                              'focal_loss_gamma': 1.5,\n",
      "                              'huber_loss_delta': 0.1,\n",
      "                              'l2_weight_decay': 0.0001,\n",
      "                              'loss_weight': 1.0},\n",
      "                'max_num_eval_detections': 100,\n",
      "                'model': {   'anchor': {   'anchor_size': 4.0,\n",
      "                                           'aspect_ratios': [0.5, 1.0, 2.0],\n",
      "                                           'num_scales': 3},\n",
      "                             'backbone': {   'resnet': {   'bn_trainable': True,\n",
      "                                                           'depth_multiplier': 1.0,\n",
      "                                                           'model_id': 50,\n",
      "                                                           'replace_stem_max_pool': False,\n",
      "                                                           'resnetd_shortcut': False,\n",
      "                                                           'scale_stem': True,\n",
      "                                                           'se_ratio': 0.0,\n",
      "                                                           'stem_type': 'v0',\n",
      "                                                           'stochastic_depth_drop_rate': 0.0},\n",
      "                                             'type': 'resnet'},\n",
      "                             'decoder': {   'fpn': {   'fusion_type': 'sum',\n",
      "                                                       'num_filters': 256,\n",
      "                                                       'use_keras_layer': False,\n",
      "                                                       'use_separable_conv': False},\n",
      "                                            'type': 'fpn'},\n",
      "                             'detection_generator': {   'apply_nms': True,\n",
      "                                                        'box_coder_weights': None,\n",
      "                                                        'max_num_detections': 100,\n",
      "                                                        'nms_iou_threshold': 0.5,\n",
      "                                                        'nms_version': 'v2',\n",
      "                                                        'pre_nms_score_threshold': 0.05,\n",
      "                                                        'pre_nms_top_k': 5000,\n",
      "                                                        'return_decoded': None,\n",
      "                                                        'soft_nms_sigma': None,\n",
      "                                                        'tflite_post_processing': {   'detections_per_class': 5,\n",
      "                                                                                      'h_scale': 1.0,\n",
      "                                                                                      'max_classes_per_detection': 11,\n",
      "                                                                                      'max_detections': 200,\n",
      "                                                                                      'nms_iou_threshold': 0.5,\n",
      "                                                                                      'nms_score_threshold': 0.1,\n",
      "                                                                                      'normalize_anchor_coordinates': False,\n",
      "                                                                                      'omit_nms': False,\n",
      "                                                                                      'use_regular_nms': False,\n",
      "                                                                                      'w_scale': 1.0,\n",
      "                                                                                      'x_scale': 1.0,\n",
      "                                                                                      'y_scale': 1.0},\n",
      "                                                        'use_class_agnostic_nms': False,\n",
      "                                                        'use_cpu_nms': False},\n",
      "                             'head': {   'attribute_heads': [],\n",
      "                                         'num_convs': 4,\n",
      "                                         'num_filters': 256,\n",
      "                                         'share_classification_heads': False,\n",
      "                                         'share_level_convs': True,\n",
      "                                         'use_separable_conv': False},\n",
      "                             'input_size': [128, 128, 3],\n",
      "                             'max_level': 7,\n",
      "                             'min_level': 3,\n",
      "                             'norm_activation': {   'activation': 'relu',\n",
      "                                                    'norm_epsilon': 0.001,\n",
      "                                                    'norm_momentum': 0.99,\n",
      "                                                    'use_sync_bn': False},\n",
      "                             'num_classes': 11},\n",
      "                'name': None,\n",
      "                'per_category_metrics': False,\n",
      "                'train_data': {   'apply_tf_data_service_before_batching': False,\n",
      "                                  'autotune_algorithm': None,\n",
      "                                  'block_length': 1,\n",
      "                                  'cache': False,\n",
      "                                  'cycle_length': None,\n",
      "                                  'decoder': {   'simple_decoder': {   'attribute_names': [   ],\n",
      "                                                                       'mask_binarize_threshold': None,\n",
      "                                                                       'regenerate_source_id': False},\n",
      "                                                 'type': 'simple_decoder'},\n",
      "                                  'deterministic': None,\n",
      "                                  'drop_remainder': True,\n",
      "                                  'dtype': 'float32',\n",
      "                                  'enable_shared_tf_data_service_between_parallel_trainers': False,\n",
      "                                  'enable_tf_data_service': False,\n",
      "                                  'file_type': 'tfrecord',\n",
      "                                  'global_batch_size': 128,\n",
      "                                  'input_path': '/home/atallah/tensorflow_datasets/cifar10/3.0.2/cifar10-train.tfrecord-00000-of-00001',\n",
      "                                  'is_training': True,\n",
      "                                  'parser': {   'aug_policy': None,\n",
      "                                                'aug_rand_hflip': True,\n",
      "                                                'aug_rand_jpeg': None,\n",
      "                                                'aug_scale_max': 1.0,\n",
      "                                                'aug_scale_min': 1.0,\n",
      "                                                'aug_type': None,\n",
      "                                                'keep_aspect_ratio': True,\n",
      "                                                'match_threshold': 0.5,\n",
      "                                                'max_num_instances': 100,\n",
      "                                                'num_channels': 3,\n",
      "                                                'pad': True,\n",
      "                                                'skip_crowd_during_training': True,\n",
      "                                                'unmatched_threshold': 0.5},\n",
      "                                  'prefetch_buffer_size': None,\n",
      "                                  'ram_budget': None,\n",
      "                                  'seed': None,\n",
      "                                  'sharding': True,\n",
      "                                  'shuffle_buffer_size': 10000,\n",
      "                                  'tf_data_service_address': None,\n",
      "                                  'tf_data_service_job_name': None,\n",
      "                                  'tfds_as_supervised': False,\n",
      "                                  'tfds_data_dir': '',\n",
      "                                  'tfds_name': '',\n",
      "                                  'tfds_skip_decoding_feature': '',\n",
      "                                  'tfds_split': '',\n",
      "                                  'trainer_id': None,\n",
      "                                  'weights': None},\n",
      "                'use_coco_metrics': True,\n",
      "                'use_wod_metrics': False,\n",
      "                'validation_data': {   'apply_tf_data_service_before_batching': False,\n",
      "                                       'autotune_algorithm': None,\n",
      "                                       'block_length': 1,\n",
      "                                       'cache': False,\n",
      "                                       'cycle_length': None,\n",
      "                                       'decoder': {   'simple_decoder': {   'attribute_names': [   ],\n",
      "                                                                            'mask_binarize_threshold': None,\n",
      "                                                                            'regenerate_source_id': False},\n",
      "                                                      'type': 'simple_decoder'},\n",
      "                                       'deterministic': None,\n",
      "                                       'drop_remainder': True,\n",
      "                                       'dtype': 'float32',\n",
      "                                       'enable_shared_tf_data_service_between_parallel_trainers': False,\n",
      "                                       'enable_tf_data_service': False,\n",
      "                                       'file_type': 'tfrecord',\n",
      "                                       'global_batch_size': 128,\n",
      "                                       'input_path': '/home/atallah/tensorflow_datasets/cifar10/3.0.2/cifar10-test.tfrecord-00000-of-00001',\n",
      "                                       'is_training': False,\n",
      "                                       'parser': {   'aug_policy': None,\n",
      "                                                     'aug_rand_hflip': False,\n",
      "                                                     'aug_rand_jpeg': None,\n",
      "                                                     'aug_scale_max': 1.0,\n",
      "                                                     'aug_scale_min': 1.0,\n",
      "                                                     'aug_type': None,\n",
      "                                                     'keep_aspect_ratio': True,\n",
      "                                                     'match_threshold': 0.5,\n",
      "                                                     'max_num_instances': 100,\n",
      "                                                     'num_channels': 3,\n",
      "                                                     'pad': True,\n",
      "                                                     'skip_crowd_during_training': True,\n",
      "                                                     'unmatched_threshold': 0.5},\n",
      "                                       'prefetch_buffer_size': None,\n",
      "                                       'ram_budget': None,\n",
      "                                       'seed': None,\n",
      "                                       'sharding': True,\n",
      "                                       'shuffle_buffer_size': 10000,\n",
      "                                       'tf_data_service_address': None,\n",
      "                                       'tf_data_service_job_name': None,\n",
      "                                       'tfds_as_supervised': False,\n",
      "                                       'tfds_data_dir': '',\n",
      "                                       'tfds_name': '',\n",
      "                                       'tfds_skip_decoding_feature': '',\n",
      "                                       'tfds_split': '',\n",
      "                                       'trainer_id': None,\n",
      "                                       'weights': None}},\n",
      "    'trainer': {   'allow_tpu_summary': False,\n",
      "                   'best_checkpoint_eval_metric': '',\n",
      "                   'best_checkpoint_export_subdir': '',\n",
      "                   'best_checkpoint_metric_comp': 'higher',\n",
      "                   'checkpoint_interval': 100,\n",
      "                   'continuous_eval_timeout': 3600,\n",
      "                   'eval_tf_function': True,\n",
      "                   'eval_tf_while_loop': False,\n",
      "                   'loss_upper_bound': 1000000.0,\n",
      "                   'max_to_keep': 5,\n",
      "                   'optimizer_config': {   'ema': None,\n",
      "                                           'learning_rate': {   'cosine': {   'alpha': 0.0,\n",
      "                                                                              'decay_steps': 1000,\n",
      "                                                                              'initial_learning_rate': 0.1,\n",
      "                                                                              'name': 'CosineDecay',\n",
      "                                                                              'offset': 0},\n",
      "                                                                'type': 'cosine'},\n",
      "                                           'optimizer': {   'sgd': {   'clipnorm': None,\n",
      "                                                                       'clipvalue': None,\n",
      "                                                                       'decay': 0.0,\n",
      "                                                                       'global_clipnorm': None,\n",
      "                                                                       'momentum': 0.9,\n",
      "                                                                       'name': 'SGD',\n",
      "                                                                       'nesterov': False},\n",
      "                                                            'type': 'sgd'},\n",
      "                                           'warmup': {   'linear': {   'name': 'linear',\n",
      "                                                                       'warmup_learning_rate': 0.05,\n",
      "                                                                       'warmup_steps': 100},\n",
      "                                                         'type': 'linear'}},\n",
      "                   'preemption_on_demand_checkpoint': True,\n",
      "                   'recovery_begin_steps': 0,\n",
      "                   'recovery_max_trials': 0,\n",
      "                   'steps_per_loop': 100,\n",
      "                   'summary_interval': 100,\n",
      "                   'train_steps': 1000,\n",
      "                   'train_tf_function': True,\n",
      "                   'train_tf_while_loop': True,\n",
      "                   'validation_interval': 100,\n",
      "                   'validation_steps': 100,\n",
      "                   'validation_summary_subdir': 'validation'}}\n"
     ]
    },
    {
     "data": {
      "application/javascript": "google.colab.output.setIframeHeight(\"500px\");",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pp.pprint(exp_config.as_dict())\n",
    "display.Javascript('google.colab.output.setIframeHeight(\"500px\");')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "43a3f868",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/device:CPU:0', '/device:GPU:0']\n",
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0',)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0',)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "logical_device_names = [logical_device.name for logical_device in tf.config.list_logical_devices()]\n",
    "print(logical_device_names)\n",
    "\n",
    "if exp_config.runtime.mixed_precision_dtype == tf.float16:\n",
    "    tf.keras.mixed_precision.set_global_policy('mixed_float16')\n",
    "\n",
    "if 'GPU' in ''.join(logical_device_names):\n",
    "  distribution_strategy = tf.distribute.MirroredStrategy()\n",
    "\n",
    "else:\n",
    "  print('Warning: this will be really slow.')\n",
    "  distribution_strategy = tf.distribute.OneDeviceStrategy(logical_device_names[0])\n",
    "\n",
    "print('Done')\n",
    "\n",
    "with distribution_strategy.scope():\n",
    "  task = tfm.core.task_factory.get_task(exp_config.task, logging_dir=model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "338b1968",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "restoring or initializing model...\n",
      "INFO:tensorflow:Customized initialization is done through the passed `init_fn`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Customized initialization is done through the passed `init_fn`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train | step:      0 | training until step 100...\n",
      "INFO:tensorflow:Error reported to Coordinator: Exception encountered when calling layer 'retina_net_model_2' (type RetinaNetModel).\n",
      "\n",
      "in user code:\n",
      "\n",
      "    File \"/home/atallah/miniconda3/envs/tensorflow18/lib/python3.12/site-packages/official/vision/modeling/retinanet_model.py\", line 129, in call  *\n",
      "        features = self.backbone(images)\n",
      "    File \"/home/atallah/miniconda3/envs/tensorflow18/lib/python3.12/site-packages/tf_keras/src/utils/traceback_utils.py\", line 70, in error_handler  **\n",
      "        raise e.with_traceback(filtered_tb) from None\n",
      "    File \"/home/atallah/miniconda3/envs/tensorflow18/lib/python3.12/site-packages/tf_keras/src/engine/input_spec.py\", line 298, in assert_input_compatibility\n",
      "        raise ValueError(\n",
      "\n",
      "    ValueError: Input 0 of layer \"res_net_2\" is incompatible with the layer: expected shape=(None, 32, 32, 3), found shape=(128, 128, 128, 3)\n",
      "\n",
      "\n",
      "Call arguments received by layer 'retina_net_model_2' (type RetinaNetModel):\n",
      "  • images=tf.Tensor(shape=(128, 128, 128, 3), dtype=float32)\n",
      "  • image_shape=None\n",
      "  • anchor_boxes=None\n",
      "  • output_intermediate_features=False\n",
      "  • training=True\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/atallah/miniconda3/envs/tensorflow18/lib/python3.12/site-packages/tensorflow/python/training/coordinator.py\", line 293, in stop_on_exception\n",
      "    yield\n",
      "  File \"/home/atallah/miniconda3/envs/tensorflow18/lib/python3.12/site-packages/tensorflow/python/distribute/mirrored_run.py\", line 387, in run\n",
      "    self.main_result = self.main_fn(*self.main_args, **self.main_kwargs)\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/__autograph_generated_filehrdst4qo.py\", line 30, in step_fn\n",
      "    logs = ag__.converted_call(ag__.ld(task_train_step), (ag__.ld(inputs),), dict(model=ag__.ld(self).model, optimizer=ag__.ld(self).optimizer, metrics=ag__.ld(self).train_metrics), fscope_1)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/atallah/miniconda3/envs/tensorflow18/lib/python3.12/site-packages/tensorflow/python/autograph/impl/api.py\", line 439, in converted_call\n",
      "    result = converted_f(*effective_args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/__autograph_generated_filegp08_53_.py\", line 24, in tf__train_step\n",
      "    outputs = ag__.converted_call(ag__.ld(model), (ag__.ld(features),), dict(training=True), fscope)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/atallah/miniconda3/envs/tensorflow18/lib/python3.12/site-packages/tensorflow/python/autograph/impl/api.py\", line 377, in converted_call\n",
      "    return _call_unconverted(f, args, kwargs, options)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/atallah/miniconda3/envs/tensorflow18/lib/python3.12/site-packages/tensorflow/python/autograph/impl/api.py\", line 459, in _call_unconverted\n",
      "    return f(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/atallah/miniconda3/envs/tensorflow18/lib/python3.12/site-packages/tf_keras/src/utils/traceback_utils.py\", line 70, in error_handler\n",
      "    raise e.with_traceback(filtered_tb) from None\n",
      "  File \"/tmp/__autograph_generated_filepciat_8d.py\", line 47, in tf__call\n",
      "    features = ag__.converted_call(ag__.ld(self).backbone, (ag__.ld(images),), None, fscope)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "ValueError: Exception encountered when calling layer 'retina_net_model_2' (type RetinaNetModel).\n",
      "\n",
      "in user code:\n",
      "\n",
      "    File \"/home/atallah/miniconda3/envs/tensorflow18/lib/python3.12/site-packages/official/vision/modeling/retinanet_model.py\", line 129, in call  *\n",
      "        features = self.backbone(images)\n",
      "    File \"/home/atallah/miniconda3/envs/tensorflow18/lib/python3.12/site-packages/tf_keras/src/utils/traceback_utils.py\", line 70, in error_handler  **\n",
      "        raise e.with_traceback(filtered_tb) from None\n",
      "    File \"/home/atallah/miniconda3/envs/tensorflow18/lib/python3.12/site-packages/tf_keras/src/engine/input_spec.py\", line 298, in assert_input_compatibility\n",
      "        raise ValueError(\n",
      "\n",
      "    ValueError: Input 0 of layer \"res_net_2\" is incompatible with the layer: expected shape=(None, 32, 32, 3), found shape=(128, 128, 128, 3)\n",
      "\n",
      "\n",
      "Call arguments received by layer 'retina_net_model_2' (type RetinaNetModel):\n",
      "  • images=tf.Tensor(shape=(128, 128, 128, 3), dtype=float32)\n",
      "  • image_shape=None\n",
      "  • anchor_boxes=None\n",
      "  • output_intermediate_features=False\n",
      "  • training=True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Error reported to Coordinator: Exception encountered when calling layer 'retina_net_model_2' (type RetinaNetModel).\n",
      "\n",
      "in user code:\n",
      "\n",
      "    File \"/home/atallah/miniconda3/envs/tensorflow18/lib/python3.12/site-packages/official/vision/modeling/retinanet_model.py\", line 129, in call  *\n",
      "        features = self.backbone(images)\n",
      "    File \"/home/atallah/miniconda3/envs/tensorflow18/lib/python3.12/site-packages/tf_keras/src/utils/traceback_utils.py\", line 70, in error_handler  **\n",
      "        raise e.with_traceback(filtered_tb) from None\n",
      "    File \"/home/atallah/miniconda3/envs/tensorflow18/lib/python3.12/site-packages/tf_keras/src/engine/input_spec.py\", line 298, in assert_input_compatibility\n",
      "        raise ValueError(\n",
      "\n",
      "    ValueError: Input 0 of layer \"res_net_2\" is incompatible with the layer: expected shape=(None, 32, 32, 3), found shape=(128, 128, 128, 3)\n",
      "\n",
      "\n",
      "Call arguments received by layer 'retina_net_model_2' (type RetinaNetModel):\n",
      "  • images=tf.Tensor(shape=(128, 128, 128, 3), dtype=float32)\n",
      "  • image_shape=None\n",
      "  • anchor_boxes=None\n",
      "  • output_intermediate_features=False\n",
      "  • training=True\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/atallah/miniconda3/envs/tensorflow18/lib/python3.12/site-packages/tensorflow/python/training/coordinator.py\", line 293, in stop_on_exception\n",
      "    yield\n",
      "  File \"/home/atallah/miniconda3/envs/tensorflow18/lib/python3.12/site-packages/tensorflow/python/distribute/mirrored_run.py\", line 387, in run\n",
      "    self.main_result = self.main_fn(*self.main_args, **self.main_kwargs)\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/__autograph_generated_filehrdst4qo.py\", line 30, in step_fn\n",
      "    logs = ag__.converted_call(ag__.ld(task_train_step), (ag__.ld(inputs),), dict(model=ag__.ld(self).model, optimizer=ag__.ld(self).optimizer, metrics=ag__.ld(self).train_metrics), fscope_1)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/atallah/miniconda3/envs/tensorflow18/lib/python3.12/site-packages/tensorflow/python/autograph/impl/api.py\", line 439, in converted_call\n",
      "    result = converted_f(*effective_args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/__autograph_generated_filegp08_53_.py\", line 24, in tf__train_step\n",
      "    outputs = ag__.converted_call(ag__.ld(model), (ag__.ld(features),), dict(training=True), fscope)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/atallah/miniconda3/envs/tensorflow18/lib/python3.12/site-packages/tensorflow/python/autograph/impl/api.py\", line 377, in converted_call\n",
      "    return _call_unconverted(f, args, kwargs, options)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/atallah/miniconda3/envs/tensorflow18/lib/python3.12/site-packages/tensorflow/python/autograph/impl/api.py\", line 459, in _call_unconverted\n",
      "    return f(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/atallah/miniconda3/envs/tensorflow18/lib/python3.12/site-packages/tf_keras/src/utils/traceback_utils.py\", line 70, in error_handler\n",
      "    raise e.with_traceback(filtered_tb) from None\n",
      "  File \"/tmp/__autograph_generated_filepciat_8d.py\", line 47, in tf__call\n",
      "    features = ag__.converted_call(ag__.ld(self).backbone, (ag__.ld(images),), None, fscope)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "ValueError: Exception encountered when calling layer 'retina_net_model_2' (type RetinaNetModel).\n",
      "\n",
      "in user code:\n",
      "\n",
      "    File \"/home/atallah/miniconda3/envs/tensorflow18/lib/python3.12/site-packages/official/vision/modeling/retinanet_model.py\", line 129, in call  *\n",
      "        features = self.backbone(images)\n",
      "    File \"/home/atallah/miniconda3/envs/tensorflow18/lib/python3.12/site-packages/tf_keras/src/utils/traceback_utils.py\", line 70, in error_handler  **\n",
      "        raise e.with_traceback(filtered_tb) from None\n",
      "    File \"/home/atallah/miniconda3/envs/tensorflow18/lib/python3.12/site-packages/tf_keras/src/engine/input_spec.py\", line 298, in assert_input_compatibility\n",
      "        raise ValueError(\n",
      "\n",
      "    ValueError: Input 0 of layer \"res_net_2\" is incompatible with the layer: expected shape=(None, 32, 32, 3), found shape=(128, 128, 128, 3)\n",
      "\n",
      "\n",
      "Call arguments received by layer 'retina_net_model_2' (type RetinaNetModel):\n",
      "  • images=tf.Tensor(shape=(128, 128, 128, 3), dtype=float32)\n",
      "  • image_shape=None\n",
      "  • anchor_boxes=None\n",
      "  • output_intermediate_features=False\n",
      "  • training=True\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"/home/atallah/miniconda3/envs/tensorflow18/lib/python3.12/site-packages/orbit/utils/loop_fns.py\", line 120, in loop_fn  *\n        step_fn(iterator)\n    File \"/home/atallah/miniconda3/envs/tensorflow18/lib/python3.12/site-packages/official/core/base_trainer.py\", line 391, in step_fn  *\n        logs = task_train_step(\n    File \"/home/atallah/miniconda3/envs/tensorflow18/lib/python3.12/site-packages/official/vision/tasks/retinanet.py\", line 327, in train_step  *\n        outputs = model(features, training=True)\n    File \"/home/atallah/miniconda3/envs/tensorflow18/lib/python3.12/site-packages/tf_keras/src/utils/traceback_utils.py\", line 70, in error_handler  **\n        raise e.with_traceback(filtered_tb) from None\n    File \"/tmp/__autograph_generated_filepciat_8d.py\", line 47, in tf__call\n        features = ag__.converted_call(ag__.ld(self).backbone, (ag__.ld(images),), None, fscope)\n\n    ValueError: Exception encountered when calling layer 'retina_net_model_2' (type RetinaNetModel).\n    \n    in user code:\n    \n        File \"/home/atallah/miniconda3/envs/tensorflow18/lib/python3.12/site-packages/official/vision/modeling/retinanet_model.py\", line 129, in call  *\n            features = self.backbone(images)\n        File \"/home/atallah/miniconda3/envs/tensorflow18/lib/python3.12/site-packages/tf_keras/src/utils/traceback_utils.py\", line 70, in error_handler  **\n            raise e.with_traceback(filtered_tb) from None\n        File \"/home/atallah/miniconda3/envs/tensorflow18/lib/python3.12/site-packages/tf_keras/src/engine/input_spec.py\", line 298, in assert_input_compatibility\n            raise ValueError(\n    \n        ValueError: Input 0 of layer \"res_net_2\" is incompatible with the layer: expected shape=(None, 32, 32, 3), found shape=(128, 128, 128, 3)\n    \n    \n    Call arguments received by layer 'retina_net_model_2' (type RetinaNetModel):\n      • images=tf.Tensor(shape=(128, 128, 128, 3), dtype=float32)\n      • image_shape=None\n      • anchor_boxes=None\n      • output_intermediate_features=False\n      • training=True\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model, eval_logs \u001b[38;5;241m=\u001b[39m \u001b[43mtfm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcore\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_lib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_experiment\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdistribution_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdistribution_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain_and_eval\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexp_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrun_post_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/tensorflow18/lib/python3.12/site-packages/official/core/train_lib.py:372\u001b[0m, in \u001b[0;36mrun_experiment\u001b[0;34m(distribution_strategy, task, mode, params, model_dir, run_post_eval, save_summary, train_actions, eval_actions, trainer, controller_cls, summary_manager, eval_summary_manager, enable_async_checkpointing)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Runs train/eval configured by the experiment params.\u001b[39;00m\n\u001b[1;32m    326\u001b[0m \n\u001b[1;32m    327\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    354\u001b[0m \u001b[38;5;124;03m      otherwise, returns {}.\u001b[39;00m\n\u001b[1;32m    355\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    356\u001b[0m runner \u001b[38;5;241m=\u001b[39m OrbitExperimentRunner(\n\u001b[1;32m    357\u001b[0m     distribution_strategy\u001b[38;5;241m=\u001b[39mdistribution_strategy,\n\u001b[1;32m    358\u001b[0m     task\u001b[38;5;241m=\u001b[39mtask,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    370\u001b[0m     enable_async_checkpointing\u001b[38;5;241m=\u001b[39menable_async_checkpointing,\n\u001b[1;32m    371\u001b[0m )\n\u001b[0;32m--> 372\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrunner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/tensorflow18/lib/python3.12/site-packages/official/core/train_lib.py:271\u001b[0m, in \u001b[0;36mOrbitExperimentRunner.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    269\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontroller\u001b[38;5;241m.\u001b[39mtrain(steps\u001b[38;5;241m=\u001b[39mparams\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mtrain_steps)\n\u001b[1;32m    270\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain_and_eval\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m--> 271\u001b[0m   \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontroller\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_and_evaluate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m      \u001b[49m\u001b[43mtrain_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m      \u001b[49m\u001b[43meval_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidation_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m      \u001b[49m\u001b[43meval_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidation_interval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124meval\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    276\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontroller\u001b[38;5;241m.\u001b[39mevaluate(steps\u001b[38;5;241m=\u001b[39mparams\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mvalidation_steps)\n",
      "File \u001b[0;32m~/miniconda3/envs/tensorflow18/lib/python3.12/site-packages/orbit/controller.py:393\u001b[0m, in \u001b[0;36mController.train_and_evaluate\u001b[0;34m(self, train_steps, eval_steps, eval_interval)\u001b[0m\n\u001b[1;32m    391\u001b[0m interval \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(train_steps \u001b[38;5;241m-\u001b[39m current_step, eval_interval)\n\u001b[1;32m    392\u001b[0m num_steps \u001b[38;5;241m=\u001b[39m current_step \u001b[38;5;241m+\u001b[39m interval\n\u001b[0;32m--> 393\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43msteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_steps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheckpoint_at_completion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    394\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevaluate(steps\u001b[38;5;241m=\u001b[39meval_steps)\n\u001b[1;32m    395\u001b[0m current_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mglobal_step\u001b[38;5;241m.\u001b[39mnumpy()\n",
      "File \u001b[0;32m~/miniconda3/envs/tensorflow18/lib/python3.12/site-packages/orbit/controller.py:282\u001b[0m, in \u001b[0;36mController.train\u001b[0;34m(self, steps, checkpoint_at_completion)\u001b[0m\n\u001b[1;32m    279\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m current_step \u001b[38;5;241m<\u001b[39m steps:\n\u001b[1;32m    280\u001b[0m   \u001b[38;5;66;03m# Calculates steps to run for the next train loop.\u001b[39;00m\n\u001b[1;32m    281\u001b[0m   num_steps \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(steps \u001b[38;5;241m-\u001b[39m current_step, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps_per_loop)\n\u001b[0;32m--> 282\u001b[0m   \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_train_n_steps\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_steps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    283\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_save_checkpoint()\n\u001b[1;32m    284\u001b[0m   current_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mglobal_step\u001b[38;5;241m.\u001b[39mnumpy()\n",
      "File \u001b[0;32m~/miniconda3/envs/tensorflow18/lib/python3.12/site-packages/orbit/controller.py:517\u001b[0m, in \u001b[0;36mController._train_n_steps\u001b[0;34m(self, num_steps)\u001b[0m\n\u001b[1;32m    515\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39msummary\u001b[38;5;241m.\u001b[39mrecord_if(should_record):\n\u001b[1;32m    516\u001b[0m     num_steps_tensor \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mconvert_to_tensor(num_steps, dtype\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mint32)\n\u001b[0;32m--> 517\u001b[0m     train_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_steps_tensor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    519\u001b[0m \u001b[38;5;66;03m# Verify that global_step was updated properly, then update current_step.\u001b[39;00m\n\u001b[1;32m    520\u001b[0m expected_step \u001b[38;5;241m=\u001b[39m current_step \u001b[38;5;241m+\u001b[39m num_steps\n",
      "File \u001b[0;32m~/miniconda3/envs/tensorflow18/lib/python3.12/site-packages/orbit/standard_runner.py:146\u001b[0m, in \u001b[0;36mStandardTrainer.train\u001b[0;34m(self, num_steps)\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_train_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    144\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_train_iter \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mnest\u001b[38;5;241m.\u001b[39mmap_structure(\u001b[38;5;28miter\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_dataset)\n\u001b[0;32m--> 146\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_train_loop_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_train_iter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_steps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    147\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_loop_end()\n",
      "File \u001b[0;32m~/miniconda3/envs/tensorflow18/lib/python3.12/site-packages/tensorflow/python/util/traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m--> 153\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    155\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/tmp/__autograph_generated_file9l71otgo.py:42\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__loop_fn\u001b[0;34m(iterator, num_steps)\u001b[0m\n\u001b[1;32m     40\u001b[0m         ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(step_fn), (ag__\u001b[38;5;241m.\u001b[39mld(iterator),), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[1;32m     41\u001b[0m _ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mUndefined(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 42\u001b[0m \u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfor_stmt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconverted_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtf\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrange\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_steps\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfscope\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloop_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mget_state_1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mset_state_1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43miterate_names\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m_\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/tmp/__autograph_generated_file9l71otgo.py:40\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__loop_fn.<locals>.loop_body\u001b[0;34m(itr)\u001b[0m\n\u001b[1;32m     38\u001b[0m _ \u001b[38;5;241m=\u001b[39m itr\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ag__\u001b[38;5;241m.\u001b[39mld(tf)\u001b[38;5;241m.\u001b[39mname_scope(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m---> 40\u001b[0m     \u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconverted_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep_fn\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfscope\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/tmp/__autograph_generated_filehrdst4qo.py:34\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_step\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m     32\u001b[0m         ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mglobal_step\u001b[38;5;241m.\u001b[39massign_add, (\u001b[38;5;241m1\u001b[39m,), \u001b[38;5;28;01mNone\u001b[39;00m, fscope_1)\n\u001b[1;32m     33\u001b[0m inputs \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mnext_train_inputs, (ag__\u001b[38;5;241m.\u001b[39mld(iterator),), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[0;32m---> 34\u001b[0m \u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconverted_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstrategy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep_fn\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_runtime_options\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfscope\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/tmp/__autograph_generated_filehrdst4qo.py:30\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_step.<locals>.step_fn\u001b[0;34m(inputs)\u001b[0m\n\u001b[1;32m     28\u001b[0m task_train_step \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mUndefined(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtask_train_step\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     29\u001b[0m ag__\u001b[38;5;241m.\u001b[39mif_stmt(ag__\u001b[38;5;241m.\u001b[39mand_(\u001b[38;5;28;01mlambda\u001b[39;00m: ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mruntime\u001b[38;5;241m.\u001b[39menable_xla, \u001b[38;5;28;01mlambda\u001b[39;00m: ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mruntime\u001b[38;5;241m.\u001b[39mnum_gpus \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m), if_body, else_body, get_state, set_state, (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtask_train_step\u001b[39m\u001b[38;5;124m'\u001b[39m,), \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 30\u001b[0m logs \u001b[38;5;241m=\u001b[39m \u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconverted_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask_train_step\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetrics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_metrics\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfscope_1\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m_train_loss\u001b[38;5;241m.\u001b[39mupdate_state, (ag__\u001b[38;5;241m.\u001b[39mld(logs)[ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mtask\u001b[38;5;241m.\u001b[39mloss],), \u001b[38;5;28;01mNone\u001b[39;00m, fscope_1)\n\u001b[1;32m     32\u001b[0m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mglobal_step\u001b[38;5;241m.\u001b[39massign_add, (\u001b[38;5;241m1\u001b[39m,), \u001b[38;5;28;01mNone\u001b[39;00m, fscope_1)\n",
      "File \u001b[0;32m/tmp/__autograph_generated_filegp08_53_.py:24\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_step\u001b[0;34m(self, inputs, model, optimizer, metrics)\u001b[0m\n\u001b[1;32m     22\u001b[0m num_replicas \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(tf)\u001b[38;5;241m.\u001b[39mdistribute\u001b[38;5;241m.\u001b[39mget_strategy, (), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\u001b[38;5;241m.\u001b[39mnum_replicas_in_sync\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ag__\u001b[38;5;241m.\u001b[39mld(tf)\u001b[38;5;241m.\u001b[39mGradientTape() \u001b[38;5;28;01mas\u001b[39;00m tape:\n\u001b[0;32m---> 24\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconverted_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtraining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfscope\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(tf)\u001b[38;5;241m.\u001b[39mnest\u001b[38;5;241m.\u001b[39mmap_structure, (ag__\u001b[38;5;241m.\u001b[39mautograph_artifact(\u001b[38;5;28;01mlambda\u001b[39;00m x: ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(tf)\u001b[38;5;241m.\u001b[39mcast, (ag__\u001b[38;5;241m.\u001b[39mld(x), ag__\u001b[38;5;241m.\u001b[39mld(tf)\u001b[38;5;241m.\u001b[39mfloat32), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)), ag__\u001b[38;5;241m.\u001b[39mld(outputs)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[1;32m     26\u001b[0m     loss, cls_loss, box_loss, model_loss \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mbuild_losses, (), \u001b[38;5;28mdict\u001b[39m(outputs\u001b[38;5;241m=\u001b[39mag__\u001b[38;5;241m.\u001b[39mld(outputs), labels\u001b[38;5;241m=\u001b[39mag__\u001b[38;5;241m.\u001b[39mld(labels), aux_losses\u001b[38;5;241m=\u001b[39mag__\u001b[38;5;241m.\u001b[39mld(model)\u001b[38;5;241m.\u001b[39mlosses), fscope)\n",
      "File \u001b[0;32m~/miniconda3/envs/tensorflow18/lib/python3.12/site-packages/tf_keras/src/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/tmp/__autograph_generated_filepciat_8d.py:47\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__call\u001b[0;34m(self, images, image_shape, anchor_boxes, output_intermediate_features, training)\u001b[0m\n\u001b[1;32m     45\u001b[0m retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mUndefinedReturnValue()\n\u001b[1;32m     46\u001b[0m outputs \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m---> 47\u001b[0m features \u001b[38;5;241m=\u001b[39m \u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconverted_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackbone\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfscope\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mget_state\u001b[39m():\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ()\n",
      "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/home/atallah/miniconda3/envs/tensorflow18/lib/python3.12/site-packages/orbit/utils/loop_fns.py\", line 120, in loop_fn  *\n        step_fn(iterator)\n    File \"/home/atallah/miniconda3/envs/tensorflow18/lib/python3.12/site-packages/official/core/base_trainer.py\", line 391, in step_fn  *\n        logs = task_train_step(\n    File \"/home/atallah/miniconda3/envs/tensorflow18/lib/python3.12/site-packages/official/vision/tasks/retinanet.py\", line 327, in train_step  *\n        outputs = model(features, training=True)\n    File \"/home/atallah/miniconda3/envs/tensorflow18/lib/python3.12/site-packages/tf_keras/src/utils/traceback_utils.py\", line 70, in error_handler  **\n        raise e.with_traceback(filtered_tb) from None\n    File \"/tmp/__autograph_generated_filepciat_8d.py\", line 47, in tf__call\n        features = ag__.converted_call(ag__.ld(self).backbone, (ag__.ld(images),), None, fscope)\n\n    ValueError: Exception encountered when calling layer 'retina_net_model_2' (type RetinaNetModel).\n    \n    in user code:\n    \n        File \"/home/atallah/miniconda3/envs/tensorflow18/lib/python3.12/site-packages/official/vision/modeling/retinanet_model.py\", line 129, in call  *\n            features = self.backbone(images)\n        File \"/home/atallah/miniconda3/envs/tensorflow18/lib/python3.12/site-packages/tf_keras/src/utils/traceback_utils.py\", line 70, in error_handler  **\n            raise e.with_traceback(filtered_tb) from None\n        File \"/home/atallah/miniconda3/envs/tensorflow18/lib/python3.12/site-packages/tf_keras/src/engine/input_spec.py\", line 298, in assert_input_compatibility\n            raise ValueError(\n    \n        ValueError: Input 0 of layer \"res_net_2\" is incompatible with the layer: expected shape=(None, 32, 32, 3), found shape=(128, 128, 128, 3)\n    \n    \n    Call arguments received by layer 'retina_net_model_2' (type RetinaNetModel):\n      • images=tf.Tensor(shape=(128, 128, 128, 3), dtype=float32)\n      • image_shape=None\n      • anchor_boxes=None\n      • output_intermediate_features=False\n      • training=True\n"
     ]
    }
   ],
   "source": [
    "model, eval_logs = tfm.core.train_lib.run_experiment(\n",
    "    distribution_strategy=distribution_strategy,\n",
    "    task=task,\n",
    "    mode='train_and_eval',\n",
    "    params=exp_config,\n",
    "    model_dir=model_dir,\n",
    "    run_post_eval=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3878f315",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow18",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
